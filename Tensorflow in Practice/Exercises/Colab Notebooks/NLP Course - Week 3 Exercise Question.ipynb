{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of NLP Course - Week 3 Exercise Question.ipynb","provenance":[{"file_id":"https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/NLP%20Course%20-%20Week%203%20Exercise%20Question.ipynb","timestamp":1578116460344},{"file_id":"1bEdiniaTOd2jRxqGDoluQKXTRmYGJUy0","timestamp":1559930374235}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hmA6EzkQJ5jt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":62},"outputId":"a75f761b-a72f-49d6-b9b8-9f4cf99f6134","executionInfo":{"status":"ok","timestamp":1578116323601,"user_tz":-330,"elapsed":4124,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["import json\n","import tensorflow as tf\n","import csv\n","import random\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import regularizers\n","\n","\n","embedding_dim = 100\n","max_length = 16\n","trunc_type='post'\n","padding_type='post'\n","oov_tok = \"<OOV>\"\n","training_size=160000\n","#Your dataset size here. Experiment using smaller values (i.e. 16000), but don't forget to train on at least 160000 to see the best effects\n","test_portion=.1\n","\n","corpus = []\n"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"bM0l_dORKqE0","colab_type":"code","outputId":"7dc7e142-3d9b-4c2e-d7d6-ebb0d1051c6d","executionInfo":{"status":"ok","timestamp":1578116355176,"user_tz":-330,"elapsed":21745,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["\n","# Note that I cleaned the Stanford dataset to remove LATIN1 encoding to make it easier for Python CSV reader\n","# You can do that yourself with:\n","# iconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv\n","# I then hosted it on my site to make it easier to use in this notebook\n","\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\\n","    -O /tmp/training_cleaned.csv\n","\n","num_sentences = 0\n","\n","# with open(\"/tmp/training_cleaned.csv\") as csvfile:\n","#     reader = csv.reader(csvfile, delimiter=',')\n","#     for row in reader:\n","#         list_item = []\n","#       # Your Code here. Create list items where the first item is the text, found in row[5], and the second is the label. Note that the label is a '0' or a '4' in the text. When it's the former, make\n","#       # your label to be 0, otherwise 1. Keep a count of the number of sentences in num_sentences\n","#         temp_label = 0\n","#         if row[0] == \"0\":\n","#           temp_label = 0\n","#         elif row[0] == \"4\":\n","#           temp_label = 4\n","#         list_item=[row[5],temp_label]\n","#         # YOUR CODE HERE\n","#         num_sentences = num_sentences + 1\n","#         corpus.append(list_item)\n","with open(\"/tmp/training_cleaned.csv\") as csvfile:\n","    reader = csv.reader(csvfile, delimiter=',')\n","    for row in reader:\n","        list_item=[]\n","        list_item.append(row[5])\n","        this_label=row[0]\n","        if this_label=='0':\n","            list_item.append(0)\n","        else:\n","            list_item.append(1)\n","        num_sentences = num_sentences + 1\n","        corpus.append(list_item)\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2020-01-04 05:38:56--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 2404:6800:4008:c04::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 238942690 (228M) [application/octet-stream]\n","Saving to: ‘/tmp/training_cleaned.csv’\n","\n","/tmp/training_clean 100%[===================>] 227.87M  44.3MB/s    in 5.1s    \n","\n","2020-01-04 05:39:07 (44.3 MB/s) - ‘/tmp/training_cleaned.csv’ saved [238942690/238942690]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3kxblBUjEUX-","colab_type":"code","outputId":"dd729af9-065d-47ab-89f1-b60dc7b9c698","executionInfo":{"status":"ok","timestamp":1578116380011,"user_tz":-330,"elapsed":4441,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["print(num_sentences)\n","print(len(corpus))\n","print(corpus[1])\n","#print(corpus)\n","\n","# Expected Output:\n","# 1600000\n","# 1600000\n","# [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]"],"execution_count":3,"outputs":[{"output_type":"stream","text":["1600000\n","1600000\n","[\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ohOGz24lsNAD","colab_type":"code","colab":{}},"source":["sentences=[]\n","labels=[]\n","random.shuffle(corpus)\n","for x in range(training_size):\n","    sentences.append(corpus[x][0])\n","    labels.append(corpus[x][1])\n","\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","\n","word_index = tokenizer.word_index\n","vocab_size=len(word_index)\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences=sequences,maxlen=max_length,padding=padding_type,truncating=trunc_type)\n","\n","split = int(test_portion * training_size)\n","\n","test_sequences = padded[0:split]\n","training_sequences = padded[split:]\n","test_labels = labels[0:split]\n","training_labels = labels[split:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIrtRem1En3N","colab_type":"code","outputId":"1b574727-5d5f-4ea9-985f-cb8312ae7159","executionInfo":{"status":"ok","timestamp":1578116397292,"user_tz":-330,"elapsed":860,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print(vocab_size)\n","print(word_index['i'])\n","# Expected Output\n","# 138858\n","# 1"],"execution_count":5,"outputs":[{"output_type":"stream","text":["138900\n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C1zdgJkusRh0","colab_type":"code","outputId":"b74c7d69-ea40-4ae3-97a0-b5601adb183c","executionInfo":{"status":"ok","timestamp":1578116419528,"user_tz":-330,"elapsed":18499,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["# Note this is the 100 dimension version of GloVe from Stanford\n","# I unzipped and hosted it on my site to make this notebook easier\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n","    -O /tmp/glove.6B.100d.txt\n","embeddings_index = {};\n","with open('/tmp/glove.6B.100d.txt') as f:\n","    for line in f:\n","        values = line.split();\n","        word = values[0];\n","        coefs = np.asarray(values[1:], dtype='float32');\n","        embeddings_index[word] = coefs;\n","\n","embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word);\n","    if embedding_vector is not None:\n","        embeddings_matrix[i] = embedding_vector;"],"execution_count":6,"outputs":[{"output_type":"stream","text":["--2020-01-04 05:40:02--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt\n","Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.187.128, 2404:6800:4008:c06::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.187.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 347116733 (331M) [text/plain]\n","Saving to: ‘/tmp/glove.6B.100d.txt’\n","\n","/tmp/glove.6B.100d. 100%[===================>] 331.04M  56.4MB/s    in 5.9s    \n","\n","2020-01-04 05:40:08 (56.4 MB/s) - ‘/tmp/glove.6B.100d.txt’ saved [347116733/347116733]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"71NLk_lpFLNt","colab_type":"code","outputId":"214991d1-1962-4c54-a80b-6519d3c509c5","executionInfo":{"status":"ok","timestamp":1578116422218,"user_tz":-330,"elapsed":1211,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(len(embeddings_matrix))\n","# Expected Output\n","# 138859"],"execution_count":7,"outputs":[{"output_type":"stream","text":["138901\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iKKvbuEBOGFz","colab":{"base_uri":"https://localhost:8080/","height":558},"outputId":"c7945606-8c31-4af1-b1eb-879efa9bb48b"},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n","    # YOUR CODE HERE - experiment with combining different types, such as convolutions and LSTMs\n","    tf.keras.layers.Dropout(rate=0.4),\n","    tf.keras.layers.Conv1D(64,5,activation=\"relu\"),\n","    tf.keras.layers.MaxPooling1D(4),\n","    tf.keras.layers.LSTM(32),\n","    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n","])\n","model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n","model.summary()\n","\n","num_epochs = 50\n","history = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=2)\n","\n","print(\"Training Complete\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 16, 100)           13890100  \n","_________________________________________________________________\n","dropout (Dropout)            (None, 16, 100)           0         \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, 12, 64)            32064     \n","_________________________________________________________________\n","max_pooling1d (MaxPooling1D) (None, 3, 64)             0         \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 32)                12416     \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 33        \n","=================================================================\n","Total params: 13,934,613\n","Trainable params: 44,513\n","Non-trainable params: 13,890,100\n","_________________________________________________________________\n","Train on 144000 samples, validate on 16000 samples\n","Epoch 1/50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qxju4ItJKO8F","colab_type":"code","colab":{}},"source":["import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['acc']\n","val_acc=history.history['val_acc']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r')\n","plt.plot(epochs, val_acc, 'b')\n","plt.title('Training and validation accuracy')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n","\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r')\n","plt.plot(epochs, val_loss, 'b')\n","plt.title('Training and validation loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend([\"Loss\", \"Validation Loss\"])\n","\n","plt.figure()\n","\n","\n","# Expected Output\n","# A chart where the validation loss does not increase sharply!"],"execution_count":0,"outputs":[]}]}